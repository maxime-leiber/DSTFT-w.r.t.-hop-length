{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from math import ceil, pi\n",
    "import copy \n",
    "\n",
    "#import IPython\n",
    "#import librosa    \n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io.wavfile as wav\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import torchaudio\n",
    "from IPython.display import Audio\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.io import loadmat\n",
    "#from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "#from torchaudio.datasets import LIBRISPEECH, YESNO, SPEECHCOMMANDS\n",
    "#from torchaudio.transforms import Spectrogram\n",
    "from torch.nn.functional import kl_div\n",
    "\n",
    "sns.set_style('white') #plt.style.use('seaborn-dark-palette')\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = 250\n",
    "mpl.rc('figure', figsize=(12, 8))\n",
    "mpl.rc('font', size=20)\n",
    "mpl.use(\"pgf\")\n",
    "mpl.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinNormSolver:\n",
    "    MAX_ITER = 250\n",
    "    STOP_CRIT = 1e-5\n",
    "\n",
    "    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n",
    "        \"\"\"\n",
    "        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n",
    "        d is the distance (objective) optimzed\n",
    "        v1v1 = <x1,x1>\n",
    "        v1v2 = <x1,x2>\n",
    "        v2v2 = <x2,x2>\n",
    "        \"\"\"\n",
    "        if v1v2 >= v1v1:\n",
    "            # Case: Fig 1, third column\n",
    "            gamma = 0.999\n",
    "            cost = v1v1\n",
    "            return gamma, cost\n",
    "        if v1v2 >= v2v2:\n",
    "            # Case: Fig 1, first column\n",
    "            gamma = 0.001\n",
    "            cost = v2v2\n",
    "            return gamma, cost\n",
    "        # Case: Fig 1, second column\n",
    "        gamma = -1.0 * ( (v1v2 - v2v2) / (v1v1+v2v2 - 2*v1v2) )\n",
    "        cost = v2v2 + gamma*(v1v2 - v2v2)\n",
    "        return gamma, cost\n",
    "\n",
    "    def _min_norm_2d(vecs, dps):\n",
    "        \"\"\"\n",
    "        Find the minimum norm solution as combination of two points\n",
    "        This is correct only in 2D\n",
    "        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n",
    "        \"\"\"\n",
    "        dmin = 1e8\n",
    "        sol = None \n",
    "        for i in range(len(vecs)):\n",
    "            for j in range(i+1,len(vecs)):\n",
    "                if (i,j) not in dps:\n",
    "                    dps[(i, j)] = 0.0\n",
    "                    for k in range(len(vecs[i])):\n",
    "                        dps[(i,j)] += torch.mul(vecs[i][k], vecs[j][k]).sum().data.cpu()\n",
    "                    dps[(j, i)] = dps[(i, j)]\n",
    "                if (i,i) not in dps:\n",
    "                    dps[(i, i)] = 0.0\n",
    "                    for k in range(len(vecs[i])):\n",
    "                        dps[(i,i)] += torch.mul(vecs[i][k], vecs[i][k]).sum().data.cpu()\n",
    "                if (j,j) not in dps:\n",
    "                    dps[(j, j)] = 0.0   \n",
    "                    for k in range(len(vecs[i])):\n",
    "                        dps[(j, j)] += torch.mul(vecs[j][k], vecs[j][k]).sum().data.cpu()\n",
    "                \n",
    "                c,d = MinNormSolver._min_norm_element_from2(dps[(i,i)], dps[(i,j)], dps[(j,j)])\n",
    "                if d < dmin:\n",
    "                    dmin = d\n",
    "                    sol = [(i,j), c, d]\n",
    "                \n",
    "        return sol, dps\n",
    "\n",
    "    def _projection2simplex(y):\n",
    "        \"\"\"\n",
    "        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        sorted_y = np.flip(np.sort(y), axis=0)\n",
    "        tmpsum = 0.0\n",
    "        tmax_f = (np.sum(y) - 1.0)/m\n",
    "        for i in range(m-1):\n",
    "            tmpsum+= sorted_y[i]\n",
    "            tmax = (tmpsum - 1)/ (i+1.0)\n",
    "            if tmax > sorted_y[i+1]:\n",
    "                tmax_f = tmax\n",
    "                break\n",
    "        return np.maximum(y - tmax_f, np.zeros(y.shape))\n",
    "    \n",
    "    def _next_point(cur_val, grad, n):\n",
    "        proj_grad = grad - ( np.sum(grad) / n )\n",
    "        tm1 = -1.0*cur_val[proj_grad<0]/proj_grad[proj_grad<0]\n",
    "        tm2 = (1.0 - cur_val[proj_grad>0])/(proj_grad[proj_grad>0])\n",
    "        \n",
    "        skippers = np.sum(tm1<1e-7) + np.sum(tm2<1e-7)\n",
    "        t = 1\n",
    "        if len(tm1[tm1>1e-7]) > 0:\n",
    "            t = np.min(tm1[tm1>1e-7])\n",
    "        if len(tm2[tm2>1e-7]) > 0:\n",
    "            t = min(t, np.min(tm2[tm2>1e-7]))\n",
    "\n",
    "        next_point = proj_grad*t + cur_val\n",
    "        next_point = MinNormSolver._projection2simplex(next_point)\n",
    "        return next_point\n",
    "\n",
    "    def find_min_norm_element(vecs):\n",
    "        \"\"\"\n",
    "        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n",
    "        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n",
    "        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n",
    "        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n",
    "        \"\"\"\n",
    "        # Solution lying at the combination of two points\n",
    "        dps = {}\n",
    "\n",
    "        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n",
    "        \n",
    "        \n",
    "        n = len(vecs)\n",
    "        sol_vec = np.zeros(n)\n",
    "        sol_vec[init_sol[0][0]] = init_sol[1]\n",
    "        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n",
    "\n",
    "        if n < 3:\n",
    "            # This is optimal for n=2, so return the solution\n",
    "            return sol_vec , init_sol[2]\n",
    "    \n",
    "        iter_count = 0\n",
    "\n",
    "        grad_mat = np.zeros((n,n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                grad_mat[i,j] = dps[(i, j)]\n",
    "                \n",
    "\n",
    "        while iter_count < MinNormSolver.MAX_ITER:\n",
    "            grad_dir = -1.0*np.dot(grad_mat, sol_vec)\n",
    "            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n",
    "            # Re-compute the inner products for line search\n",
    "            v1v1 = 0.0\n",
    "            v1v2 = 0.0\n",
    "            v2v2 = 0.0\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    v1v1 += sol_vec[i]*sol_vec[j]*dps[(i,j)]\n",
    "                    v1v2 += sol_vec[i]*new_point[j]*dps[(i,j)]\n",
    "                    v2v2 += new_point[i]*new_point[j]*dps[(i,j)]\n",
    "            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n",
    "            new_sol_vec = nc*sol_vec + (1-nc)*new_point\n",
    "            change = new_sol_vec - sol_vec\n",
    "            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n",
    "                return sol_vec, nd\n",
    "            sol_vec = new_sol_vec\n",
    "        return sol_vec, nd\n",
    "\n",
    "    def find_min_norm_element_FW(vecs):\n",
    "        \"\"\"\n",
    "        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n",
    "        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n",
    "        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n",
    "        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n",
    "        \"\"\"\n",
    "        # Solution lying at the combination of two points\n",
    "        dps = {}\n",
    "        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n",
    "\n",
    "        n=len(vecs)\n",
    "        sol_vec = np.zeros(n)\n",
    "        sol_vec[init_sol[0][0]] = init_sol[1]\n",
    "        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n",
    "\n",
    "        if n < 3:\n",
    "            # This is optimal for n=2, so return the solution\n",
    "            return sol_vec , init_sol[2]\n",
    "\n",
    "        iter_count = 0\n",
    "\n",
    "        grad_mat = np.zeros((n,n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                grad_mat[i,j] = dps[(i, j)]\n",
    "\n",
    "        while iter_count < MinNormSolver.MAX_ITER:\n",
    "            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n",
    "\n",
    "            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n",
    "            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n",
    "            v2v2 = grad_mat[t_iter, t_iter]\n",
    "\n",
    "            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n",
    "            new_sol_vec = nc*sol_vec\n",
    "            new_sol_vec[t_iter] += 1 - nc\n",
    "\n",
    "            change = new_sol_vec - sol_vec\n",
    "            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n",
    "                return sol_vec, nd\n",
    "            sol_vec = new_sol_vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def gradient_normalizers(grads, losses, normalization_type):\n",
    "    gn = {}\n",
    "    if normalization_type == 'l2':\n",
    "        for t in grads:\n",
    "            gn[t] = np.sqrt(np.sum([gr.pow(2).sum().data.cpu() for gr in grads[t]]))\n",
    "    elif normalization_type == 'loss':\n",
    "        for t in grads:\n",
    "            gn[t] = losses[t]\n",
    "    elif normalization_type == 'loss+':\n",
    "        for t in grads:\n",
    "            gn[t] = losses[t] * np.sqrt(np.sum([gr.pow(2).sum().data.cpu() for gr in grads[t]]))\n",
    "    elif normalization_type == 'none':\n",
    "        for t in grads:\n",
    "            gn[t] = 1.0\n",
    "    else:\n",
    "        print('ERROR: Invalid Normalization Type')\n",
    "    return gn\n",
    "\n",
    "\n",
    "# Scaling the loss functions based on the algorithm choice\n",
    "def scale_loss(mystft, alpha, epsilon, x, tasks):\n",
    "    grads = {}\n",
    "    loss_data = {}\n",
    "    scale = {}\n",
    "    #tasks = ['entropy', 'coverage', 'kurtosis']\n",
    "    \n",
    "    params = [ {'params': mystft.parameters()}]\n",
    "    optimizer = torch.optim.RMSprop(params)\n",
    "    \n",
    "    # Compute gradients wrt to entropy \n",
    "    optimizer.zero_grad()\n",
    "    spec, _ = mystft(x)\n",
    "    dict_loss = mystft.loss(spec)\n",
    "    \n",
    "    alphat = mystft.strides[..., 1:] - (mystft.actual_win_length[..., 1:]-mystft.actual_win_length[..., 0:-1])/2   \n",
    "    betat = alphat[:, 1:]+alphat[:, :-1]\n",
    "    betat = torch.cat((2*alphat[:, 0:1], betat, 2*alphat[:, -1:]), dim=1) \n",
    "    weight = betat/betat.sum()\n",
    "    weight = torch.ones_like(weight)\n",
    "    weight = weight/weight.sum()\n",
    "    \n",
    "    entropy = 1. * (1- (dict_loss['div_js_u'] * weight).sum().mean())  \n",
    "    \n",
    "    loss_data['entropy'] = entropy.data\n",
    "    entropy.backward()\n",
    "    \n",
    "    grads['entropy'] = []\n",
    "    for param in mystft.parameters():\n",
    "        if param.grad is not None:\n",
    "                grads['entropy'].append(Variable(param.grad.data.clone(), requires_grad=False))\n",
    "    \n",
    "    # Compute gradients wrt to coverage \n",
    "    optimizer.zero_grad()\n",
    "    spec, _ = mystft(x)  \n",
    "    dict_loss = mystft.loss(spec)\n",
    "    cov = 1. * (1 - dict_loss['cov'].mean()-epsilon).pow(alpha)\n",
    "    loss_data['coverage'] = cov.data\n",
    "    cov.backward()\n",
    "    \n",
    "    grads['coverage'] = []\n",
    "    for param in mystft.parameters():\n",
    "        if param.grad is not None:\n",
    "                grads['coverage'].append(Variable(param.grad.data.clone(), requires_grad=False))\n",
    "                \n",
    "                \n",
    "    # Compute gradients wrt to kurtosis \n",
    "    optimizer.zero_grad()\n",
    "    spec, _ = mystft(x)  \n",
    "    dict_loss = mystft.loss(spec)\n",
    "    #kur =  torch.exp( - 0.01 * dict_loss['kurtosis'].mean() )\n",
    "    kur = 1 / dict_loss['kurtosis'].mean() \n",
    "    loss_data['kurtosis'] = kur.data\n",
    "    kur.backward()\n",
    "    \n",
    "    grads['kurtosis'] = []\n",
    "    for param in mystft.parameters():\n",
    "        if param.grad is not None:\n",
    "                grads['kurtosis'].append(Variable(param.grad.data.clone(), requires_grad=False))\n",
    "                \n",
    "        \n",
    "    \n",
    "    \n",
    "    # Normalize all gradients \n",
    "    \n",
    "    \n",
    "    my_tasks = copy.deepcopy(tasks)\n",
    "    removed_tasks = []\n",
    "    my_grads = {}\n",
    "    my_loss_data = {}\n",
    "    \n",
    "    \n",
    "    for t in tasks:\n",
    "        if (loss_data[t] <=1e-6):\n",
    "            my_tasks.remove(t)\n",
    "            removed_tasks.append(t)\n",
    "        else:\n",
    "            my_grads[t] = grads[t]\n",
    "            my_loss_data[t] = loss_data[t]\n",
    "    \n",
    "    if len(my_tasks)>1:\n",
    "        gn = gradient_normalizers(my_grads, my_loss_data, 'loss+')\n",
    "\n",
    "        #print('norm', gn)\n",
    "\n",
    "        for t in my_tasks:\n",
    "            for gr_i in range(len(my_grads[t])):\n",
    "                my_grads[t][gr_i] = my_grads[t][gr_i] / gn[t]\n",
    "\n",
    "                \n",
    "        # Frank-Wolfe iteration to compute scales.\n",
    "        sol, min_norm =  MinNormSolver.find_min_norm_element([my_grads[t] for t in my_tasks])\n",
    "    \n",
    "        for i, t in enumerate(my_tasks):\n",
    "            scale[t] = float(sol[i])\n",
    "            \n",
    "        \n",
    "    else: # Only one task\n",
    "        \n",
    "        for i, t in enumerate(my_tasks):\n",
    "            scale[t] = 1.0\n",
    "\n",
    "    for i, t in enumerate(removed_tasks):\n",
    "            scale[t] = 0.0\n",
    "            \n",
    "    return scale\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 1e3\n",
    "f = 100 * (torch.ones(600) + .1 * torch.rand(600))\n",
    "#f = torch.cat((f, torch.linspace(100, 200, 300)))\n",
    "f = torch.cat((f, 200 * (torch.ones(700) + .1 * torch.rand(700))))\n",
    "#f = torch.cat((f, torch.linspace(200, 50, 400)))\n",
    "f = torch.cat((f, 50 * (torch.ones(500) + .1 * torch.rand(500))))\n",
    "#f = torch.cat((f, torch.linspace(50, 250, 100)))\n",
    "f = torch.cat((f, 250 * (torch.ones(600) + .1 * torch.rand(600))))\n",
    "#f = torch.cat((f, torch.linspace(250, 50, 400)))\n",
    "f = torch.cat((f, 100 * (torch.ones(1800) + .1 * torch.rand(1800))))\n",
    "#f = torch.cat((f, torch.linspace(50, 300, 200)))\n",
    "f = torch.cat((f, 300 * (torch.ones(1000) + .1 * torch.rand(1000))))\n",
    "\n",
    "\n",
    "\n",
    "f = f + 0.*torch.ones_like(f)* torch.randn_like(f)\n",
    "plt.plot(f)\n",
    "plt.show()\n",
    "\n",
    "x = torch.sin(2*pi*torch.cumsum(f, 0)/sr)\n",
    "\n",
    "x += 0.3 * torch.randn(x.shape)\n",
    "x = x[None, :].to(device)\n",
    "mark_list = [0, 600, 1300, 1800, 2400, 4200, 5199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MySpec(nn.Module):\n",
    "    def __init__(self, x, win_length: float, support: int, stride: float, pow: float = 1.0, requires_grad: bool = True, \n",
    "                    tapering_function: str = 'hann', dynamic_parameter : bool = False, window_transform=None, stride_transform=None):\n",
    "        super(MySpec, self).__init__()\n",
    "        \n",
    "        # Constants and hyperparameters\n",
    "        self.N = support                        # support size\n",
    "        self.F = int(1 + self.N/2)              # nb of frequencies\n",
    "        self.B = x.shape[0]                     # batch size\n",
    "        self.L = x.shape[-1]                    # signal length\n",
    "        self.device = x.device          \n",
    "        self.dtype = x.dtype\n",
    "        self.requires_grad = requires_grad      \n",
    "        self.tapering_function = tapering_function\n",
    "        self.dynamic_parameter = dynamic_parameter\n",
    "        self.pow = pow\n",
    "        self.tap_win = None\n",
    "        #self.inf = 0\n",
    "        #self.sup = self.L-self.N\n",
    "        # Register eps and min as a buffer tensor\n",
    "        self.register_buffer('eps', torch.tensor(torch.finfo(torch.float).eps, dtype=self.dtype, device=self.device))\n",
    "        self.register_buffer('min', torch.tensor(torch.finfo(torch.float).min, dtype=self.dtype, device=self.device))\n",
    "        # Definie number of frames \n",
    "        self.T = int(torch.div(self.L, stride, rounding_mode='floor'))       #int(1 + torch.div(self.L - (self.N - 1) - 1, stride, rounding_mode='floor'))      \n",
    "        \n",
    "        # Learnable parameters        \n",
    "        #self.register_parameter('strides', nn.Parameter(torch.full((self.T,), abs(stride), dtype=self.dtype, device=self.device), requires_grad=self.requires_grad))\n",
    "\n",
    "        \n",
    "        if window_transform is None:\n",
    "            self.window_transform = self.__window_transform\n",
    "        else:\n",
    "            self.window_transform = window_transform\n",
    "        \n",
    "        if stride_transform is None:\n",
    "            self.stride_transform = self.__stride_transform\n",
    "        else:\n",
    "            self.stride_transform = stride_transform\n",
    "        \n",
    "        strides = torch.full((self.T,), abs(stride), dtype=self.dtype, device=self.device)\n",
    "        strides[0] = 0\n",
    "        self.strides = nn.Parameter(strides, requires_grad=self.requires_grad)  \n",
    "        self.win_length = nn.Parameter(torch.full((1, self.T,), abs(win_length), dtype=self.dtype, device=self.device), requires_grad=self.requires_grad)\n",
    "        \n",
    "    def __window_transform(self, w):\n",
    "        wout = torch.minimum(torch.maximum(w, self.N/20*torch.ones_like(w, device=self.device)), self.N*torch.ones_like(self.win_length, device=device))\n",
    "        return wout\n",
    "    \n",
    "    def __stride_transform(self, s):\n",
    "        sout = torch.minimum(torch.maximum(s, 0*torch.ones_like(s, device=device)), 2*self.N*torch.ones_like(s, device=device))\n",
    "        sout[0] = s[0]\n",
    "        return sout \n",
    "        \n",
    "    @property \n",
    "    def actual_win_length(self):\n",
    "        return self.window_transform(self.win_length)\n",
    "\n",
    "    @property \n",
    "    def actual_strides(self):\n",
    "        return self.stride_transform(self.strides)\n",
    "    \n",
    "        \n",
    "    @property \n",
    "    def raw_strides(self):\n",
    "        # Compute the strides between frames (support) \n",
    "        raw_strides = self.actual_strides + torch.cat((torch.tensor([self.N], dtype=self.dtype, device=self.device), self.actual_win_length.squeeze())).diff()/2\n",
    "        return raw_strides\n",
    "        \n",
    "    @property\n",
    "    def frames(self):\n",
    "        # Compute the temporal position (indices) od frames (support)        \n",
    "        frames = torch.cumsum(self.raw_strides, dim=0)   \n",
    "        return frames\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Compute the short-time Fourier transform of the input signal\n",
    "        stft = self.stft(x, 'forward')\n",
    "        spec = stft.abs().pow(self.pow)[:, :self.F]\n",
    "        return spec + self.eps, stft \n",
    "    \n",
    "    def backward(self, x, dl_ds):\n",
    "        # Compute the gradient of the criteria with respect to learnable parameters\n",
    "        dstft_dp = self.stft(x, 'backward')\n",
    "        dl_dp = (torch.conj(dl_ds) * dstft_dp).sum().real\n",
    "        return dl_dp.unsqueeze(0)\n",
    "    \n",
    "    def stft(self, x: torch.tensor, direction: str):\n",
    "        # Fourier coefficients \n",
    "        coeff = torch.arange(end=self.N, device=self.device, dtype=self.dtype)\n",
    "        coeff = coeff[:, None] @ coeff[None, :]\n",
    "        coeff = torch.exp(- 2j * pi * coeff / self.N)     \n",
    "\n",
    "        # frames index\n",
    "        idx_floor = self.frames.floor()       \n",
    "        idx_frac = self.frames - idx_floor \n",
    "        idx_floor = idx_floor.long()[:, None].expand((self.T, self.N)) + torch.arange(0, self.N, device=self.device)\n",
    "        idx_floor[idx_floor>=self.L] = -1\n",
    "        strided_x = x[:, idx_floor]\n",
    "        strided_x[:, idx_floor < 0 ] = 0 \n",
    "\n",
    "        \n",
    "        self.tap_win = self.window_function(direction=direction, idx_frac=idx_frac).permute(2, 1, 0)\n",
    "        \n",
    "        strided_x = strided_x[:, :, None, :]\n",
    "        \n",
    "        self.tap_win = self.tap_win[None, :, :, :]\n",
    "        coeff = coeff[None, None, :, :]        \n",
    "        \n",
    "        tapered_x = (strided_x * self.tap_win)[:, :, :, :, None]\n",
    "        tapered_x = torch.view_as_complex(torch.cat((tapered_x, torch.zeros_like(tapered_x)), -1))\n",
    "        \n",
    "        stft = (tapered_x * coeff).sum(dim=-1)\n",
    "        stft = stft.permute(0, 2, 1)\n",
    "        \n",
    "        return stft\n",
    "        \n",
    "    def window_function(self, direction: str, idx_frac) -> torch.tensor:\n",
    "        \n",
    "        valid_window_functions = {'hann', 'hanning', }\n",
    "        if self.tapering_function not in valid_window_functions:\n",
    "            raise ValueError(f\"tapering_function must be one of '{valid_window_functions}', but got padding_mode='{self.tapering_function}'\")\n",
    "        else:\n",
    "            base = torch.arange(0, self.N, 1, dtype=self.dtype, device=self.device)[:, None, None].expand([-1, self.N, self.T])    \n",
    "            base = base - idx_frac\n",
    "            self.expanded_win_length = self.actual_win_length[:, None, :].expand([self.N, self.N, self.T])\n",
    "        \n",
    "            self.tap_win = 0.5 - 0.5 * torch.cos(2 * pi * (base + (self.expanded_win_length-self.N+1)/2) / self.expanded_win_length )  \n",
    "            \n",
    "            \n",
    "            if (base.shape[-1] == self.actual_win_length.shape[-1]) or self.actual_win_length.shape[-1] == 1:\n",
    "                mask1 = base.ge(torch.ceil( (self.N-1+self.actual_win_length)/2))\n",
    "                mask2 = base.le(torch.floor((self.N-1-self.actual_win_length)/2))            \n",
    "            else:\n",
    "                mask1 = base.ge(torch.ceil( (self.N-1+self.actual_win_length.permute(-1, -2))/2))\n",
    "                mask2 = base.le(torch.floor((self.N-1-self.actual_win_length.permute(-1, -2))/2))\n",
    "            self.tap_win[mask1] = 0\n",
    "            self.tap_win[mask2] = 0\n",
    "        \n",
    "        self.tap_win = self.tap_win / self.tap_win.sum(dim=0, keepdim=True)\n",
    "        return self.tap_win\n",
    "        \n",
    "    def loss(self, spec,):\n",
    "        dict_loss = {}\n",
    "        \n",
    "        x = spec.permute(0, 2, 1) # B, T, F\n",
    "        probs = torch.div(x.permute(0, 2, 1), x.sum(dim=-1)).permute(0, 2, 1) # B, T, F\n",
    "        \n",
    "        # compute divergence Jensen-Shannon between frame and uniform distribution       \n",
    "        p, q = probs, torch.full_like(probs, 1/self.F/np.log(2)) \n",
    "        m = (p + q) / 2\n",
    "        div_pm = (p * (torch.clamp(torch.log(p), min=self.min) - torch.clamp(torch.log(m), min=self.min))).sum(dim=-1)\n",
    "        div_qm = (q * (torch.clamp(torch.log(q), min=self.min) - torch.clamp(torch.log(m), min=self.min))).sum(dim=-1)\n",
    "        div_js_u = (div_pm + div_qm) / 2\n",
    "        dict_loss['div_js_u'] = div_js_u.mean()\n",
    "        \n",
    "        \n",
    "        # compute kurtosis\n",
    "        kur = (x - x.mean(dim=-1, keepdim=True)).pow(4).mean(dim=-1) / (x - x.mean(dim=-1, keepdim=True)).pow(2).mean(dim=-1).pow(2) # B, T\n",
    "        dict_loss['kurtosis'] = kur.mean()\n",
    "        \n",
    "        # compute coverage\n",
    "        coverage = torch.minimum(self.actual_strides[1:], self.actual_win_length[..., 0:-1]) \n",
    "        coverage = torch.cat((coverage, self.actual_win_length[:, -1].unsqueeze(-1)), dim=1).squeeze()\n",
    "        init = torch.minimum(torch.maximum(torch.zeros_like(self.frames), self.frames+self.N/2-self.actual_win_length[0]/2), self.L*torch.ones_like(self.frames))\n",
    "        end = torch.maximum(torch.minimum(self.L*torch.ones_like(self.frames), self.frames+self.N/2+ torch.minimum( self.actual_win_length[0]/2, - self.actual_win_length[0]/2+coverage )), torch.zeros_like(self.frames))\n",
    "        \n",
    "        cov = torch.sum(end-init) / self.L\n",
    "        dict_loss['cov'] = cov\n",
    "        \n",
    "        \n",
    "        return dict_loss\n",
    "    \n",
    "    def print(self, spec, loss=None, marklist=None):\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(18, 7))\n",
    "        gs = GridSpec(1, 2, figure=fig)\n",
    "        \n",
    "        if loss is not None:\n",
    "            title = \"\"\n",
    "            for key, value in loss.items():\n",
    "                #print(key, value)\n",
    "                title += f' {key} {value:.2f} - '\n",
    "            plt.suptitle(title)\n",
    "        \n",
    "        ax1 = fig.add_subplot(gs[0])\n",
    "        ax2 = fig.add_subplot(gs[1])\n",
    "        \n",
    "        # Plotting the log-spectrogram\n",
    "        ax1.imshow(spec[0].log().detach().cpu(), aspect='auto', origin='lower', cmap='jet', extent=[0, spec.shape[-1], 0, 1])\n",
    "        ax2.plot([0, self.L], [self.T, self.T], linewidth=3, c='k')\n",
    "        if marklist is not None:\n",
    "            for elem in marklist:\n",
    "                ax2.axvline(elem, 0, self.T, c='r')\n",
    "        for i, (start, length) in enumerate(zip(self.frames.detach().cpu(), self.actual_win_length[0].detach().cpu())):\n",
    "            #ax2.plot([start+self.N/2-length/2, start+self.N/2+length/2], [self.T-i-1, self.T-i-1], c='r')   \n",
    "            #ax2.plot([start, start+self.N], [self.T-i-1.3, self.T-i-1.3], c='k')    \n",
    "            if self.tap_win is not None:\n",
    "                #ax2.plot(range(int((start+self.N/2-length/2).floor().item()), int((start+self.N/2+length/2).floor().item())), self.T-i-1.3+ 150 * self.tap_win[int((self.N/2-length/2).floor().item()):int((self.N/2+length/2).floor().item()), 0, i].squeeze().detach().cpu())\n",
    "                ax2.plot(range(int(start.floor().item()), int(start.floor().item()+self.N)), self.T-i-1.3+ 150 * self.tap_win[:, 0, i].squeeze().detach().cpu())\n",
    "            #ax2.plot(range(start, start+self.N), self.T-i-1 + self.tap_win[0, :, i])   \n",
    "            #ax2.plot(range(start, start+self.N), self.T-i-1 - self.tap_win[:, 0, i])   \n",
    "        \n",
    "        \n",
    "        plt.show()        \n",
    "        return\n",
    "    \n",
    "    def print2(self, spec, loss=None, marklist=None, x=None, epoch=None):\n",
    "        if epoch is None: epoch = 0\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        #ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "        ticks = torch.arange(200, 200+self.L, self.L/self.T)\n",
    "        labels = (self.frames.detach().cpu().numpy() + self.N/2).astype(int)\n",
    "        ax.set_xticks(ticks)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.imshow(spec[0].log().detach().cpu(), aspect='auto', origin='lower', cmap='jet', extent=[0, self.L, 0, .5])\n",
    "        #ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "        plt.savefig(f'spec_{epoch}')\n",
    "        plt.show()   \n",
    "        \n",
    "        fig = plt.figure(constrained_layout=True)\n",
    "        if x is not None:\n",
    "            plt.plot(self.T + .5 + .5 * x.squeeze().cpu(), linewidth=1, c='k')\n",
    "        else:\n",
    "            plt.plot([0, self.L], [self.T, self.T], linewidth=3, c='k')\n",
    "        plt.xticks(self.frames.cpu().detach() + self.N/2)\n",
    "        \n",
    "        #print('xticks', plt.get_xticklabels())\n",
    "        #print(self.frames.cpu().detach() + self.N/2)\n",
    "        \n",
    "        if marklist is not None:\n",
    "            for elem in marklist:\n",
    "                plt.axvline(elem, 0, self.T, c='r')\n",
    "        for i, (start, length) in enumerate(zip(self.frames.detach().cpu(), self.actual_win_length[0].detach().cpu())):\n",
    "            #ax2.plot([start+self.N/2-length/2, start+self.N/2+length/2], [self.T-i-1, self.T-i-1], c='r')   \n",
    "            #ax2.plot([start, start+self.N], [self.T-i-1.3, self.T-i-1.3], c='k')    \n",
    "            if self.tap_win is not None:\n",
    "                #ax2.plot(range(int((start+self.N/2-length/2).floor().item()), int((start+self.N/2+length/2).floor().item())), self.T-i-1.3+ 150 * self.tap_win[int((self.N/2-length/2).floor().item()):int((self.N/2+length/2).floor().item()), 0, i].squeeze().detach().cpu())\n",
    "                plt.plot(range(int(start.floor().item()), int(start.floor().item()+self.N)), self.T-i-1.3+ 150 * self.tap_win[:, 0, i].squeeze().detach().cpu(), c='b')\n",
    "        plt.savefig(f'frames_{epoch}')\n",
    "        plt.show() \n",
    "        \n",
    "        \n",
    "        indices = (self.actual_strides > 200).nonzero()    \n",
    "        indices = indices.squeeze().tolist()\n",
    "        indices.insert(0, 0)\n",
    "        print(indices)\n",
    "        print(self.frames + self.N/2)\n",
    "        print(self.actual_win_length)\n",
    "        spec2 = spec[..., indices]        \n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xticks(ticks[indices])\n",
    "        ax.set_xticklabels(labels[indices])\n",
    "        ax.imshow(spec2[0].log().detach().cpu(), aspect='auto', origin='lower', cmap='jet', extent=[0, self.L, 0, .5])\n",
    "        #ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "        plt.savefig(f'spec2_{epoch}')\n",
    "        plt.show()   \n",
    "        \n",
    "        #indices = indices.insert(0, 0)\n",
    "        #gs = GridSpec(1, 2, figure=fig)\n",
    "        #\n",
    "        #if loss is not None:\n",
    "        #    title = \"\"\n",
    "        #    for key, value in loss.items():\n",
    "        #        #print(key, value)\n",
    "        #        title += f' {key} {value:.2f} - '\n",
    "        #    plt.suptitle(title)\n",
    "        #\n",
    "        #ax1 = fig.add_subplot(gs[0])\n",
    "        #ax2 = fig.add_subplot(gs[1])\n",
    "        #\n",
    "        ## Plotting the log-spectrogram\n",
    "        #ax1.imshow(spec[0].log().detach().cpu(), aspect='auto', origin='lower', cmap='jet', extent=[0, spec.shape[-1], 0, 1])\n",
    "        #ax2.plot([0, self.L], [self.T, self.T], linewidth=3, c='k')\n",
    "        #if marklist is not None:\n",
    "        #    for elem in marklist:\n",
    "        #        ax2.axvline(elem, 0, self.T, c='r')\n",
    "        #for i, (start, length) in enumerate(zip(self.frames.detach().cpu(), self.actual_win_length[0].detach().cpu())):\n",
    "        #    #ax2.plot([start+self.N/2-length/2, start+self.N/2+length/2], [self.T-i-1, self.T-i-1], c='r')   \n",
    "        #    #ax2.plot([start, start+self.N], [self.T-i-1.3, self.T-i-1.3], c='k')    \n",
    "        #    if self.tap_win is not None:\n",
    "        #        #ax2.plot(range(int((start+self.N/2-length/2).floor().item()), int((start+self.N/2+length/2).floor().item())), self.T-i-1.3+ 150 * self.tap_win[int((self.N/2-length/2).floor().item()):int((self.N/2+length/2).floor().item()), 0, i].squeeze().detach().cpu())\n",
    "        #        ax2.plot(range(int(start.floor().item()), int(start.floor().item()+self.N)), self.T-i-1.3+ 150 * self.tap_win[:, 0, i].squeeze().detach().cpu())\n",
    "        #    #ax2.plot(range(start, start+self.N), self.T-i-1 + self.tap_win[0, :, i])   \n",
    "        #    #ax2.plot(range(start, start+self.N), self.T-i-1 - self.tap_win[:, 0, i])   \n",
    "      \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystft = MySpec(x, 900, 1500, 500)\n",
    "spec, _ = mystft(x)\n",
    "dict_loss = mystft.loss(spec)\n",
    "mystft.print2(spec, dict_loss, mark_list, x) \n",
    "\n",
    "params = [ {'params': mystft.parameters(),    'lr': 100.0}]\n",
    "opt = torch.optim.Adam(params)\n",
    "\n",
    "alpha = 1.0\n",
    "inter_epoch = 5\n",
    "win_epoch = 0\n",
    "epsilon = 0.\n",
    "\n",
    "min_kur = 1\n",
    "\n",
    "scale ={}\n",
    "all_tasks = ['coverage', 'kurtosis', 'entropy']\n",
    "tasks = ['kurtosis', 'coverage']\n",
    "for epoch in range(1_000):  \n",
    "\n",
    "    # Research scales \n",
    "    scale = scale_loss(mystft, alpha, epsilon, x, tasks)\n",
    "    for iii in list(set(all_tasks)-set(tasks)):\n",
    "        scale[iii]= 0.0\n",
    "    \n",
    "    for k in range(inter_epoch):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        \n",
    "        spec, _ = mystft(x)\n",
    "        dict_loss = mystft.loss(spec)        \n",
    "        \n",
    "        alphat = mystft.actual_strides[..., 1:] - (mystft.actual_win_length[..., 1:]-mystft.actual_win_length[..., 0:-1])/2        \n",
    "        betat = alphat[:, 1:]+alphat[:, :-1]\n",
    "        betat = torch.cat((2*alphat[:, 0:1], betat, 2*alphat[:, -1:]), dim=1) \n",
    "        weight = betat/betat.sum()\n",
    "        entropy = 1. * (1-  (dict_loss['div_js_u'] * weight).sum().mean())   \n",
    "        \n",
    "        cov = 1. * (1 - dict_loss['cov'].mean()-epsilon).pow(alpha)\n",
    "        \n",
    "        kur =  100 / dict_loss['kurtosis'].mean()  \n",
    "        #win_loss =  50 / mystft.win_length.mean()\n",
    "        \n",
    "        err = scale['entropy']*entropy + scale['coverage']*cov + scale['kurtosis']*kur #+ win_loss\n",
    "        loss =   kur\n",
    "        \n",
    "        err.backward() \n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:.0f};\\t Err {err:.2f};\\t Entr {entropy:.3f};\\t Kur {1000 * kur:.3f};\\t Cov {cov:.2f};\\t Scales {scale} \")\n",
    "    if min_kur >= kur:\n",
    "        min_kur = kur\n",
    "        print(f\"Epoch {epoch:.0f};\\t Err {err:.2f};\\t Entr {entropy:.3f};\\t Kur {1000 * kur:.3f};\\t Cov {cov:.2f};\\t Scales {scale} \")\n",
    "        if epoch > 9000 or epoch % 100 == 0:\n",
    "            mystft.print2(spec, dict_loss, mark_list, x, epoch) \n",
    "        \n",
    "    #sch.step(loss)\n",
    "    \n",
    "    if opt.param_groups[0]['lr'] < .1:\n",
    "        break\n",
    "        \n",
    "    \n",
    "\n",
    "spec, _ = mystft(x)\n",
    "spec = spec.detach()\n",
    "print(mystft.frames)\n",
    "mystft.print2(spec, dict_loss, mark_list, x, epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "607a7be30f1111a7b2d6aabbefa927e8d03597fdef7d6869627ceb4f0bfccf36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
